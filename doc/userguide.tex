\documentclass[a4paper]{article}
\usepackage[a4paper]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\title{Beam Hardening Correction CarouselFit: User Guide}
\author{Ronald Fowler}

\maketitle

\begin{abstract}
This document is a brief user guide to the Python software package CarouselFit which takes image
data from a number of known samples produced by an X-ray CT machine and fits them to a model of
the beam hardening process which occurs when a broad spectrum source is used image a sample.
This model can then be used to generate corrections appropriate for a single material to convert
the observed attenuation values into the actual attenuation that would be observed for that material
with monochromatic X-rays.
This software is based on the IDL package and ideas described in \cite{davis}.
\end{abstract}

\section{Introduction}
Beam hardening is well known problem that is described in many works, see \cite{davis}.
This software takes as input a number of images of well characterised samples and uses these
to fit a simple model of the expected beam hardening (BH) to the observed data.
The result is an estimate of the ``response function'', $R(E)$, which gives the expected output signal
from the detector as a function of X-ray energy for the selected combination of X-ray source, filters
and detector.
Note that due to aging effects of the X-ray source, detector, etc., this function may change over time,
so ideally the calibration measurements should be made before and after each CT scan.
In addition the model allows for variation in the form of $R(E)$ with the number of the scan line.
This can occur due to the way the emitted X-ray spectra is known to depend on the ``take-off'' angle.

The next section describes how to download and run the software.
In section 3 we describe how to set up the necessary files that give
information on the number and type of test images that are used for calibration and the
image formats.
Section 4 details how to the run the fitting an post-processing image modules of the software.
The first of these fits the model to the data while the second applies the correction either
directly to the CT image data or generates a look-up table to map observed attenuation values to
mono-chromatic attenuation.
Section 5 shows two examples of the use of the software.

\section{Downloading and running the software}

The software is available from the CCPForge repository.
It consists of a Python software package along with a number of data files that are used to help model the X-ray
beams and the material attenuation.
As well as a Python environment the software depends on a number of additional packages being available.
An easy way to access most of the required packages is to download the Anaconda Python environment which is
available for Linux, MacOS and Windows systems from \url{https://www.continuum.io/downloads}.
It is recommended that the user installs this before installing the CarouselFit software.
Alternatively the user may install the required packages in their local Python installation.

The CarouselFit software can be checked out to a suitable directory using the command:
\begin{verbatim}
    svn co https://ccpforge.cse.rl.ac.uk/svn/tomo_bhc/trunk carouselFit
\end{verbatim}
This will create a set of three directories under \texttt{carouselFit}:
\begin{itemize}
\item \texttt{src:} this contains the Python source code
\item \texttt{doc:} this contains documentation of the software
\item \texttt{test:} this contains several sub-directories with information on attenuation and X-ray spectra.
The source code must be executed from this directory and any updates to the carousel or crown information
should be made in the \texttt{carouselData} sub-directory.
\end{itemize}

After downloading the software the installation can be checked by running Python in the \texttt{test} directory
and reading the example script file.
On Linux and MacOS this could be done from a command line. assuming that the Anaconda version of Python is loaded into
the system PATH as:
\begin{verbatim}
  python ../src/runCarouselFit.py
  read script.short
  quit
\end{verbatim}

This set of commands should run without producing any error messages, such failure to import some modules.
If a python other than Anaconda has been used it may be necessary to install additional libraries.
Check the documentation for your Python system to see how to do this.

On Windows systems Anaconda python can be accessed from the Start Menu aster it has been installed.
The software can be downloaded using the command line version of svn, e.g. via Cygwin, or using the GUI
provided by TortoiseSVN \url{https://tortoisesvn.net/}.
Once the software is installed, start an IP Python window (or similar) from the Start Menu and navigate to the 
\texttt{test} directory as mentioned above. Then use the \texttt{\%run} command to execute the code, e.g.:
\begin{verbatim}
   cd c:/svnpath/test
   %run ../src/runCarousel.py
   read script.short
   quit
\end{verbatim}

\section{Configuration files}

The original calibration device described in \cite{davis} was called a carousal as it was built from a set of 9 test samples
arranged between two circular supports allowing for each of the samples to be imaged individually by the scanner.
The samples would cover the full range of lines in the scanner, but not the full range of each row; typically only
the centre half of each row would be covered by the sample.

A more recent calibration device has been developed at staff at the Research Centre at Harwell (RCaH) which is
known as a crown. This device allows a larger number of samples to be mounted.
In this case the sample usually covers all lines and rows of the image.

\subsection{Carousel sample definition file}

The materials mounted on the carousel, or crown, must be described in a simple ASCII file which is stored
in the \texttt{test/carouselData} directory.
An example of the format that was used for the carousel from QMUL is shown below.
\begin{verbatim}
# carousel definition file based on data from QMUL 17/11/14
10
Cu,Ti,Ti,Ti,Al,Al,Al,Al,Al,NOTHING
8.92,4.506,4.506,4.506,2.698,2.698,2.698,2.698,2.698,1
0.2093,0.4420,0.2210,0.1105,0.3976,0.1988,0.0994,0.0497,0.02,0.
\end{verbatim}

This illustrates a case where there are 9 sample materials in the carousel.
In this case all the samples are pure metals of known thickness and density.
It is important to emphasize that the calibration depends on the sample materials
been very well characterised.
If a large error exists in either the thickness or purity of a sample this can undermine
the accuracy of the fitting process.
No exact guidelines have yet been defined on the best set of test materials to use, but obviously
samples of the material the forms the dominant absorber in the imaged target would be ideal.
However, this is often not practical in many cases, such as bone and teeth studies, where calcium metal
is the prime absorber, but samples of the pure metal are subject chemical reactions in air.
As long as the energy dependence of the sample attenuation coefficient, $\mu(E)$, is not too different to that of
target dominant absorber then the calibration method should work.
Some possible problems may occur if the sample has sharp steps in $\mu(E)$ due to band edges that lie in the
response range of the system which are not seen in the target material.
For example, compare the attenuation of Sn with that of Ca in the range 0 to 75KeV.

The above file uses the simple format:
\begin{itemize}
\item{line1:} a comment line, starting with #, to describe the file
\item{line2:} a single integer giving the number of sample materials plus 1
\item{line3:} a set of comma separated strings giving the names of each sample, with no spaces. the
number of names must be the same as the previous number, with the final one named "NOTHING".
In this case the samples are all pure metals and the chemical symbol has been used as the name.
However any name be used as long as a corresponding file with the extension \texttt{.txt} exists
in the directory \texttt{test/xcom}. This file gives the energy dependent $\mu(E)$ for this sample.
\item{line4:} a set of comma separated values giving the density (in g/cm3) of each sample. A dummy
value of 1 is used for the final material.
\item{line5:} a set of comma separated values giving the thickness of each sample in cm. A dummy value of
0. is added on the end.
\end{itemize}
If a sample type other then the ones already described in \texttt{test/xcom} is used it is necessary to
create a file of the attenuation values of that sample.
See the Readme file in that directory for details.

The thickness range of the samples should aim to cover the range of attenuations that are expected in the test sample.

\subsection{Sample image data file}

In addition to a description of the samples in the carousel it is also necessary to define the format of the sample
images and details of the X-ray source, filters and detector.
This is done via another file in the directory \texttt{test/carouselData} which has the default extension \texttt{.data}.
One such file must be generated for each calibration case, while the above carousel definition file will only change
if the samples are changed.

Again a simple ASCII format is used to define the necessary values.
An example is shown below:
\begin{verbatim}
# data for one QMUL calibration run
80              # voltage
22              # angle
W               # target material
19.25           # target density
600             # image res rows
800             # image res lines
carouselData/run001.img         # image file
float32         # data type in image file
2               # number of filters
Al              # filter material
0.12            # filter width
2.698           # filter density
Cu              # filter material
0.1             # filter width - 0.1
8.92            # filter density
CsI             # detector material
0.01            # starting value for detector thickness
4.51            # detector density
\end{verbatim}

The format has one value per line with a comment to described the value.
Most of these are self describing, such as the accelerating voltage, the take-off angle,
the target material (tungsten, W) and its density, for the X-ray source.

The path to the file containing the sample images must be included in this file.
All the images must currently be in a single file.
The format used above, \texttt{float32}, assumes a binary format with 9 separate images of $600 \times 800$ 32bit floating
point values.
Each value is $log ( I_0 / I )$ for that pixel.

Another supported format is \texttt{uint16}. In this case the sample images values are unsigned 16 bit values of the $I$ value.
Again these are all packed in order in a single file. The first image of the file is the (shading corrected) flat field image.
The $I_0$ value is taken as the average of this initial image.
 
Usually a set of filters are used to limit the limit the energy range of the X-ray beam. In the case of the QMUL data they
normally employ two filters with 0.12cm of Al and 0.1cm of Cu, as shown in the above file.
As the fitting process includes varying the exact Cu filter width it is recommended that a zero width Cu filter element is included
even if no Cu was used in the actual imaging.

The definition of the detector material is important and tests to date have been made with CsI. However other materials may be used
if their attenuation profile is included in the \texttt{text/xcom} directory.
Since the width of the detector is used as a fitting parameter it is not necessary to specify an accurate starting value, though this
will be used in the command \texttt{showspec}, if it is run before a fit has been performed.

\section{The command line interface}

\subsection{Command list}

When the Python software is started from Python or a similar environment, a simple command prompt is issued.
Typing \texttt{help} will give a simple list of the available commands, though without much detail of the options.

The available commands are:
\begin{itemize}
\item{\bf read \it{filename}} This command opens the given file and reads commands from it until end of file.
Control is then returned to the command line. Do not include blank lines in the command script.
\item{\bf load \it{file.def} \it{file.data}} This reads the definition file for the carousel and the data
relating to the actual calibration images. These two files must exist and are described in the previous section.
they are normally located in the \texttt{test/carouselData} directory.
\item{\bf quit} Exit the program.
\item{\bf help} Give a list of available commands.
\item{\bf showcor \it{[l1 l2...]}} This command will plot the attenuation correction curve for any one or lines. If no arguments are given
it will plot the first, middle and last correction lines. The matplotlib zoom feature can be used to focus on a particular region of the
plot. It can only be used after a fit has been performed.
\item{\bf showimag} This command will plot the images of each sample in one window. It may be useful to check for problems with the samples.
It can only be used after data has been loaded.
\item{\bf fitatt nlines \it{[linestep]}} This command attempts to fit the model to the selected samples. The
number of lines of data to fit must be given. This maybe followed by a ``step'', e.g. 10 to use every 10th line.
This can be useful when using many lines as fitting all of them can be very slow.
\item{\bf vary \it{[target|detector|filter|energy npoly]}} On its own this command lists the order of polynomial used in fitting the line wise
dependence of each of the three main parameters, target width, detector width and filter width.
Thus setting 0 for each of these would mean no variation across lines, while setting each as 1 gives a linear variation of each in line number.
The fit time increases significantly with the order used.
Extra terms also can be added to the normally linear dependence of the response to the photon energy; however this polynomial is not constrained
to be positive and the fit may fail. Hence the vary value defaults to -1 which indicates no energy dependence.
\item{\bf initguess \it{$s_1$ $s_2$ $s_3$}} Set the initial guess to be used by fitatt.
$s_1$ is the width of the target filter (usually tungsten), $s_2$ is the log width of the detector (usually CsI) and $s-3$ is the width of the
fitted filter (usually copper). Commonly used values for the initial guess are 0.01 -6.0 0.01.
\item{\bf mask \it{[n1 n2..]}} Without arguments this shows the set of masks that control if a given sample will be used in the next fit operation.
By default all values are true which means that sample will be used in the fit. Samples are labeled from 1 to $n$ and to mask the $m$ sample
that number should be given as an argument to the mask command. A negative value can be used to unmask a previously masked sample.
\item{\bf setcormat \it{material energy}} This command should be used before a fit operation to define the material and energy to which the correction
curve should be determined. For example \texttt{setcormat Al 40} sets the correction curve to be calculated for Aluminium at 40KeV.
At present if the correction material or energy are altered it is necessary to rerun the fit command.
\item{\bf showspec \it{[line]}} - plot three spectra, the input X-ray spectrum, the filtered spectrum and the response spectrum.
If a line number is given, plots are for that line. The default is line 0.
\item{\bf showatt \it[nsamp nline]} - plot the sample attenuations along a specific line. By default this shows the
attenuation for all samples at line 400. Samples are labeled 0 to $n-1$ in this case.
\item{\bf debug} - set debugging option, for diagnostic purposes only.
\item{\bf showconf} - list some of the settings, such as the filters, detector, source and voltage.
\item{\bf setwidth \it[width]} - without arguments, prints the width, in pixels, used to average over each line to get the mean
attenuation. For the QMUL data, where the sample does not cover the whole image, it is important to ensure this does not
exceed the true sample width. For the RCaH data, where the image does cover the whole width, a larger value can be used.
\item{\bf setfilter \it[material width]} - without arguments lists the filters defined. Can also be used
to change the width of existing filters, though not add new ones. Used for debugging.

\end{itemize}

\subsection{Using the software}

As described in section 3 it is necessary to write the definition files that describe the carousel and the particular
test case that is being treated.
The latter file must also point to the data file that contains the sample images in a suitable format.
It is assumed that corrections for dark and flat field images have being applied to the images before they are
passed to the software.
A simple partial analysis might consist of the following steps:
\begin{verbatim}
    load carouselData/carousel0.def carouselData/run001.data

    showimg
    showatt
\end{verbatim}
The first command loads the definition and run data from files, while the next two commands
plot the 2D images and 1D cuts along line=400.

\begin{verbatim}
    setcormat CaHydro 40

    vary target 1
    vary detector 0
    vary filter 1

    initguess .01 -6 .0
    fitatt 799 2

    showspec
    showcor
\end{verbatim}
These commands then set the material and energy to which we wish to correct the data via the \texttt{setcormat}
command, and then alter the default orders of the fit variables.
The \texttt{fitatt} command fits the given initial guess using the lines of the image data, 0-799, but only every 2nd line.
This fit may take 60 seconds. Finally the fitted spectrum and correction curves are plotted.

The correction curves are stored in the same format as used in the earlier IDL code as separate 8th order polynomial
fits to the correction data in a file called polyfit.npz.
These curves are the ones shown by the \texttt{showcor} command above.
To actually apply the correction to image data requires the use of another Python program, \texttt{applyTrans.py}.

\subsection{applyTrans.py}

The Python script applyTrans.py can be used to update image files using the correction curves calculated by
the above fitting process.
It can also calculate a file of type \texttt{.bht} which can be used by XtekCT machines to correct the image
data used in CT analysis. In this case only one correction curve is applied to all the data.

The syntax of the command can be seen using the \texttt{-h} option, which gives:
\begin{verbatim}
applyTrans.py [-r rows -l lines -p poly.npz -w whiteLevel -x file.bht]
              [-d] [file1.ext] [filen.ext]
\end{verbatim}
In the above data it is usually necessary to specify the image size in rows and lines.
If all the image data is stored in a single file with data type float32, as used for
some data from QMUL, then the following command can be used to process it:

\begin{verbatim}
python ../src/applyTrans.py -r 600 -l 800 images.raw
\end{verbatim}
In this case the default file \texttt{polyfit.npz} is read to find the correction curves.
If 800 curves are present then one will be applied to each line in the image.
If only one correction curve is present then this one correction will be used on all image lines.
The processed output will be written to \texttt{bhc\_images.raw}.
Note that the \texttt{whiteLevel} parameter is not needed in this case as the \texttt{.raw} extension
is taken to imply \texttt{float32} data of $log(I_0/I)$.

To generate a \texttt{.bht} correction file the following command can be used:
\begin{verbatim}
python ../src/applyTrans.py -b -x xtekct.bht -w 59200
\end{verbatim}
In this case only the file \texttt{xtekct.bht} is generated. It is necessary to provide an accurate estimate
of the white level since any pixels above this are mapped to no attenuation.

\begin{thebibliography}{9}

\bibitem{davis}
  Graham R. Davis, Anthony N.Z. Evershed, and David Mills,
  \emph{Quantitative high contrast X-ray microtomography for dental research},
  Journal of Dentistry,
  Volume 41, Issue 5, May 2013, Pages 475â€“482.

\end{thebibliography}



\end{document}
